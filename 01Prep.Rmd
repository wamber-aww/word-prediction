---
title: 'Part I: Preprocessing'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
setwd('/Users/wamber/Documents/Projects/NLP/')
```

## Background

This is an extension of the [Coursera Data Science Specialization Capstone Project](https://www.coursera.org/learn/data-science-project), 
aimed at building a word prediction application. 
The Capstone Dataset can be downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

The first part of the analysis is to clean up the corpus to enable exploratory data analysis and prediction model building.

## Basics

### 1. Packages
```{r, message=F, warning=F}
suppressMessages(library(quanteda))
suppressMessages(library(readtext))
suppressMessages(library(stringi))
suppressMessages(library(kableExtra))
suppressMessages(library(ggplot2))
suppressMessages(library(cowplot))
suppressMessages(library(reshape2))
```

### 2. Read in data
```{r}
data_blog <- texts(readtext(file = "data/en_US.blogs.txt"))
data_news <- texts(readtext(file = "data/en_US.news.txt"))
data_twt <- texts(readtext(file = "data/en_US.twitter.txt"))
```

The `readtext()` function loads the text files into a `data.frame` obeject. The text can be accessed with the `text()` method. To prevent all the texts from printing (**don't use `head()`**), we can use the `stri_sub()` function to print out the desired characters (1st to 80th) of the texts of the `data_` objects. 

```{r}
stri_sub(data_blog, 1, 80)
```

### 3. Basic summary

```{r}
data_sum <- data.frame("Source" = c("Blog", "News", "Twitter"),
                       "Lines" = c(stri_count_fixed(data_blog, "\n"),
                                   stri_count_fixed(data_news, "\n"),
                                   stri_count_fixed(data_twt, "\n")))
data_sum %>% kable("html") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

We can count the lines by counting the newline characters `\n` in the text

## Preprocessing

### 1. Combining corpus
```{r}
corp_blog <- corpus(data_blog)
corp_news <- corpus(data_news)
corp_twt <- corpus(data_twt)
corp_all <- corp_blog + corp_news + corp_twt
corp_sum <- summary(corp_all)
save(corp_all, file = "RData/corp_all.RData")
save(corp_sum, file = "RData/corp_sum.RData")
```
We can first construct individual corpus from each data source and combine all data into a `corp_all` variable that is still a `corpus` object. The summary information can be calculated from the combined corpus and plot the distribution of words and sentences based on different data source

```{r}
corp_sum %>% kable("html") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Prior to any preprocessing, it seems like blogs contain the most words and Twitter the most sentences

```{r}
word <- ggplot(data = corp_sum, aes(x = Text, y = Tokens, fill = Text)) +
  geom_col() +
  guides(fill = FALSE) +
  scale_y_continuous(expand = c(0, 0)) +
  ylab('Word counts') + 
  scale_x_discrete(labels = c("Blogs", "News", "Twitter"))

sentence <- ggplot(data = corp_sum, aes(x = Text, y = Sentences, fill = Text)) +
  geom_col() +
  guides(fill = FALSE) +
  scale_y_continuous(expand = c(0, 0)) +
  ylab('Sentences') +
  scale_x_discrete(labels = c("Blogs", "News", "Twitter"))

print(plot_grid(word, sentence, labels = "AUTO"))
```

### 2. Tokenization

We first want to tokenize the words with the `tokens()` function, which splits the text into individual words. For downstream analysis, numbers, punctuations, symbols, Twitter characters, and urls are removed since they are likely not useful to the next word prediction.

```{r}
tok <- tokens(corp_all, what = "word",
              remove_numbers = TRUE, remove_punct = TRUE,
              remove_symbols = TRUE, remove_twitter = TRUE,
              remove_url = TRUE)
```

```{r}
tok_chr <- tok %>% as.character()
tok_chr <- char_tolower(tok_chr)
tok_chr[1:10]
tok_sum <- summary(tok)
save(tok_sum, file = "RData/tok_sum.RData")
```

We can access the tokens by making the tokens into a character vector, transforming to lower case, and printing out the first 10 tokenized words. Similarly, we can look at the distribution of tokenized words from each data source

```{r}
tok_sum %>% kable("html") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

We can now compare the differences in words from all sources before and after tokenization

```{r}
# Make df for tokens
tok_sum_df <- as.data.frame(tok_sum[, 1])
tok_sum_df$source <- c("Blogs", "News", "Twitter")
colnames(tok_sum_df) <- c("Length", "Source")

word_tok <- ggplot(data = tok_sum_df, aes(x = Source, y = Length, fill = Source)) +
  geom_col() +
  guides(fill = FALSE) +
  scale_y_discrete(expand = c(0, 0)) +
  ylab('Tokenized word counts') + 
  scale_x_discrete(labels = c("Blogs", "News", "Twitter"))

print(plot_grid(word, word_tok, labels = "AUTO"))
```


### 3. Profanity filtering

The next step is to remove any profanity, since we probably wouldn't want a word prediction app to suggest bad words

```{r}
if (!file.exists('badWords.txt')) {
  download.file('https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en',
                dest = paste0(getwd(), 'badWords.txt'),
                method = 'curl', quiet = T)
  }
prof <- readLines('badWords.txt', skipNul = T)
```

The list of profanity is obtained from [here](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en).

```{r}
tok <- tokens_remove(tok, pattern = prof)
```

We will directly overwrite the tokenized `tok` variable and save it for future use. Some big files will also be removed to free up some memory.

```{r}
save(tok, file = "RData/tok.RData")
rm(data_blog, data_news, data_twt)
rm(corp_blog, corp_news, corp_twt)
```

### 4. Stemming

Word stemming is the process to reduce inflected and derived words to their root form. For example, type, types, typed, typing, would all be reduced to type. We can perform stemming with th `dfm()` function that constructs a document-feature matrix to summarize the tokens

```{r}
dfm1 <- dfm(tok, tolower = TRUE, stem = TRUE)
save(dfm1, file = "RData/dfm1.RData")
```

### 5. Remove stop words

Stop words refer to the most common words in a language that is not always valuable for analysis and doesn't change the overall tone of the sentence (to, and, for, from, this, that, etc.). Removal of stop words often depend on the application. In our case, we would want to include stop words for modeling and prediction. But to explore word frequencies, we will (temporarily) remove the stop words.

```{r}
dfm1_rm <- dfm(dfm1, tolower = TRUE, remove = stopwords("english"))
save(dfm1_rm, file = "RData/dfm1_rm.RData")
```

## Summary

In this part, we covered the basics of cleaning up a text dataset to allow for downstream analysis and modeling. The steps for preprocessing the corpus was:

  1. Tokenization
  2. Profanity filtering
  3. Word stemming
  4. Stop words removal
  
We also saved the cleaned corpus as `RData` files for the next part, **Part II: Exploratory Data Analysis**

## References

  1. [quanteda](https://docs.quanteda.io/articles/pkgdown/replication/digital-humanities.html)
  2. [Basics of NLP](https://www.linkedin.com/pulse/basics-natural-language-processing-aswathi-nambiar/)